{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "f34d86a5",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Entrenamiento del modelo Gpt2\n",
    "import json\n",
    "import re\n",
    "import time\n",
    "import numpy as np\n",
    "import torch\n",
    "from torch.utils.data import DataLoader\n",
    "from transformers import GPT2Tokenizer, GPT2LMHeadModel,  Trainer, TrainingArguments, pipeline\n",
    "from sklearn.model_selection import train_test_split\n",
    "from crear_dataset import ChatDataset\n",
    "\n",
    "#Configuraciones principales\n",
    "RANDOM_STATE = 42\n",
    "LEARNING_RATE = 5e-5\n",
    "torch.manual_seed(123)\n",
    "\n",
    "torch.backends.cudnn.deterministic = True\n",
    "DEVICE = torch.device('cuda') if torch.cuda.is_available() else torch.device('cpu')\n",
    "\n",
    "#Abrir json y cargar\n",
    "with open(\"./results/astronomia_corpus.json\", \"r\", encoding=\"utf-8\") as f: #Asegurarse estar en el directorio correcto\n",
    "    docs = json.load(f)\n",
    "\n",
    "#Obtener solo el texto\n",
    "texts = [d[\"text\"] for d in docs]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "29374825",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "228 58\n"
     ]
    }
   ],
   "source": [
    "\n",
    "#Dividr los datos 80% train y 20% val\n",
    "train_texts, val_texts = train_test_split(texts, test_size=0.2, random_state=RANDOM_STATE)\n",
    "\n",
    "print(len(train_texts), len(val_texts))\n",
    "\n",
    "#Limpiar los datos/texto \n",
    "def clean_text(text):\n",
    "    text = text.lower()\n",
    "    text = re.sub(r\"http\\S+\", \"\", text)  #quitar URLs\n",
    "    text = re.sub(r\"[^a-z0-9\\s.,;:!?()\\-']\", \" \", text)  #quitar rarezas\n",
    "    text = re.sub(r\"\\s+\", \" \", text).strip()\n",
    "    return text\n",
    "\n",
    "train_texts = [clean_text(t) for t in train_texts] #textos limpios y preparados para crear datsets de estos\n",
    "eval_texts = [clean_text(t) for t in val_texts]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "b5d6785a",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\diego\\miniconda3\\envs\\astro_env\\lib\\site-packages\\transformers\\tokenization_utils_base.py:1617: FutureWarning: `clean_up_tokenization_spaces` was not set. It will be set to `True` by default. This behavior will be deprecated in transformers v4.45, and will be then set to `False` by default. For more details check this issue: https://github.com/huggingface/transformers/issues/31884\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([  732,   779,   281, 16359, 16161, 46320,  3781,  2446,   284,  5554,\n",
      "        48668,  3953,   262,  9158,  1483,   286, 27982, 10090, 14500,   355,\n",
      "        48804,   605,    13,   262,  1366,   900,   973,   329,   262,  3781,\n",
      "        10874,   286,  3126,    11, 44085, 16161,  4263,   351,  2266, 30846,\n",
      "         6492,   416,   262,  1017, 24611,  4875,  6766,  5526,   357, 21282,\n",
      "          824,     8,   290, 10090, 14500,   416, 16161, 26626,    11,   355,\n",
      "          880,   355,   262, 48321,    18,   290, 12385,   940, 18388,   947,\n",
      "           13,   356,  3953,   262,  9158,  1483,   286,   262, 27982,   416,\n",
      "         1262,   262,   308, 38200,  9107,  2446,    11,   543, 31408,   262,\n",
      "        16161,  2939,   284,   663, 44503, 12245,  7110,   284,  4886, 16161,\n",
      "         9158,  1483,   326,   318,   287,   867,  2663,  2408,   284,  4003,\n",
      "          416, 10107, 13432,   286,   262,  8246, 16161,  2939,    13, 11992,\n",
      "         2482,  1262, 14500, 10090, 48804,   605,   290,   264])\n",
      "tensor([1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "        1, 1, 1, 1, 1, 1, 1, 1])\n"
     ]
    }
   ],
   "source": [
    "#Comporbar funcionamiento del tokenizer y definir el transformador\n",
    "#Al tokenizar con tu tokenizer, se a√±ade autom√°ticamente el √≠ndice de cada token en el vocabulario del modelo.\n",
    "tokenizer = GPT2Tokenizer.from_pretrained(\"gpt2\")\n",
    "tokenizer.pad_token = tokenizer.eos_token  \n",
    "block_size = 128\n",
    "train_encodigns = tokenizer(list(train_texts), truncation = True, padding = \"max_length\", max_length = block_size, return_tensors = \"pt\")\n",
    "valid_encodings = tokenizer(list(eval_texts), truncation = True, padding = \"max_length\", max_length = block_size, return_tensors = \"pt\")\n",
    "\n",
    "print(valid_encodings['input_ids'][2])\n",
    "print(valid_encodings['attention_mask'][2])\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "d973e7e2",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Crear Dataset de entrenamiento y validaci√≥n\n",
    "train_dataset = ChatDataset(train_encodigns)\n",
    "val_dataset = ChatDataset(valid_encodings)\n",
    "\n",
    "#Crear Dataloaders de train y val datsets\n",
    "train_loader = DataLoader(train_dataset, batch_size=16, shuffle=True)\n",
    "valid_loader = DataLoader(val_dataset, batch_size=16, shuffle=False) \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "e2b4f727",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "GPT2LMHeadModel(\n",
       "  (transformer): GPT2Model(\n",
       "    (wte): Embedding(50257, 768)\n",
       "    (wpe): Embedding(1024, 768)\n",
       "    (drop): Dropout(p=0.1, inplace=False)\n",
       "    (h): ModuleList(\n",
       "      (0-11): 12 x GPT2Block(\n",
       "        (ln_1): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "        (attn): GPT2SdpaAttention(\n",
       "          (c_attn): Conv1D(nf=2304, nx=768)\n",
       "          (c_proj): Conv1D(nf=768, nx=768)\n",
       "          (attn_dropout): Dropout(p=0.1, inplace=False)\n",
       "          (resid_dropout): Dropout(p=0.1, inplace=False)\n",
       "        )\n",
       "        (ln_2): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "        (mlp): GPT2MLP(\n",
       "          (c_fc): Conv1D(nf=3072, nx=768)\n",
       "          (c_proj): Conv1D(nf=768, nx=3072)\n",
       "          (act): NewGELUActivation()\n",
       "          (dropout): Dropout(p=0.1, inplace=False)\n",
       "        )\n",
       "      )\n",
       "    )\n",
       "    (ln_f): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "  )\n",
       "  (lm_head): Linear(in_features=768, out_features=50257, bias=False)\n",
       ")"
      ]
     },
     "execution_count": 37,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Definir el modelo y el optimizador\n",
    "model = GPT2LMHeadModel.from_pretrained(\"gpt2\")\n",
    "optimizer = torch.optim.AdamW(model.parameters(), lr=LEARNING_RATE)\n",
    "model.to(DEVICE)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "8ecf2922",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\diego\\miniconda3\\envs\\astro_env\\lib\\site-packages\\transformers\\training_args.py:1545: FutureWarning: `evaluation_strategy` is deprecated and will be removed in version 4.46 of ü§ó Transformers. Use `eval_strategy` instead\n",
      "  warnings.warn(\n",
      " 33%|‚ñà‚ñà‚ñà‚ñé      | 11/33 [22:27<44:55, 122.54s/it]\n",
      "\n",
      "\n",
      "  0%|          | 0/60 [00:00<?, ?it/s]c:\\Users\\diego\\Desktop\\Informatica\\Proyectos-Propios\\astro-ai-portfolio\\4-Astronomic-Chatbot\\crear_dataset.py:16: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  item = {key: torch.tensor(val[idx]) for key, val in self.encodings.items()} #en key estan todo el texto encodeadas\n",
      " 18%|‚ñà‚ñä        | 11/60 [00:02<00:11,  4.37it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 4.2152, 'grad_norm': 6.360230922698975, 'learning_rate': 4.5e-05, 'epoch': 0.67}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                               \n",
      " 25%|‚ñà‚ñà‚ñå       | 15/60 [00:03<00:08,  5.29it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'eval_loss': 3.2432572841644287, 'eval_runtime': 0.2685, 'eval_samples_per_second': 215.997, 'eval_steps_per_second': 14.896, 'epoch': 1.0}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\diego\\Desktop\\Informatica\\Proyectos-Propios\\astro-ai-portfolio\\4-Astronomic-Chatbot\\crear_dataset.py:16: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  item = {key: torch.tensor(val[idx]) for key, val in self.encodings.items()} #en key estan todo el texto encodeadas\n",
      " 33%|‚ñà‚ñà‚ñà‚ñé      | 20/60 [00:06<00:13,  2.89it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 3.4197, 'grad_norm': 4.830862522125244, 'learning_rate': 3.6666666666666666e-05, 'epoch': 1.33}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 50%|‚ñà‚ñà‚ñà‚ñà‚ñà     | 30/60 [00:08<00:05,  5.19it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 3.2757, 'grad_norm': 8.88134765625, 'learning_rate': 2.8333333333333335e-05, 'epoch': 2.0}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                               \n",
      " 50%|‚ñà‚ñà‚ñà‚ñà‚ñà     | 30/60 [00:08<00:05,  5.19it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'eval_loss': 3.144075870513916, 'eval_runtime': 0.2329, 'eval_samples_per_second': 248.989, 'eval_steps_per_second': 17.172, 'epoch': 2.0}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\diego\\Desktop\\Informatica\\Proyectos-Propios\\astro-ai-portfolio\\4-Astronomic-Chatbot\\crear_dataset.py:16: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  item = {key: torch.tensor(val[idx]) for key, val in self.encodings.items()} #en key estan todo el texto encodeadas\n",
      " 68%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñä   | 41/60 [00:12<00:04,  4.17it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 3.1749, 'grad_norm': 4.851055145263672, 'learning_rate': 2e-05, 'epoch': 2.67}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                               \n",
      " 75%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñå  | 45/60 [00:13<00:02,  5.17it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'eval_loss': 3.110599994659424, 'eval_runtime': 0.2711, 'eval_samples_per_second': 213.957, 'eval_steps_per_second': 14.756, 'epoch': 3.0}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\diego\\Desktop\\Informatica\\Proyectos-Propios\\astro-ai-portfolio\\4-Astronomic-Chatbot\\crear_dataset.py:16: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  item = {key: torch.tensor(val[idx]) for key, val in self.encodings.items()} #en key estan todo el texto encodeadas\n",
      " 85%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñå | 51/60 [00:16<00:02,  3.22it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 3.1269, 'grad_norm': 4.764472007751465, 'learning_rate': 1.1666666666666668e-05, 'epoch': 3.33}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 60/60 [00:18<00:00,  5.16it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 3.1207, 'grad_norm': 8.884801864624023, 'learning_rate': 3.3333333333333333e-06, 'epoch': 4.0}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\diego\\Desktop\\Informatica\\Proyectos-Propios\\astro-ai-portfolio\\4-Astronomic-Chatbot\\crear_dataset.py:16: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  item = {key: torch.tensor(val[idx]) for key, val in self.encodings.items()} #en key estan todo el texto encodeadas\n",
      "                                               \n",
      "100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 60/60 [00:20<00:00,  5.16it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'eval_loss': 3.0999505519866943, 'eval_runtime': 0.245, 'eval_samples_per_second': 236.737, 'eval_steps_per_second': 16.327, 'epoch': 4.0}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 60/60 [00:21<00:00,  2.75it/s]\n",
      "c:\\Users\\diego\\Desktop\\Informatica\\Proyectos-Propios\\astro-ai-portfolio\\4-Astronomic-Chatbot\\crear_dataset.py:16: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  item = {key: torch.tensor(val[idx]) for key, val in self.encodings.items()} #en key estan todo el texto encodeadas\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'train_runtime': 21.8054, 'train_samples_per_second': 41.824, 'train_steps_per_second': 2.752, 'train_loss': 3.3888439814249676, 'epoch': 4.0}\n",
      "Total Training Time: 0.37 min\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 4/4 [00:00<00:00, 18.87it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Evaluation results: {'eval_loss': 3.0999505519866943, 'eval_runtime': 0.229, 'eval_samples_per_second': 253.277, 'eval_steps_per_second': 17.467, 'epoch': 4.0}\n",
      "Perplexity: 22.20\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "#Para agilizar el entrenamiento y hacerlo m√°s visual, se entrena mediante el Trianer de tranforms y de esta manera sabemos que el modelo se entran bien\n",
    "#La didferencia entre entranr got2 y no hacerlo con los datsets ,es que l ainformacion que ofrece es distinta a la entrenada y se puede invetar cosas que no son\n",
    "#Definir los argumentos de√± entrenamiento\n",
    "training_args = TrainingArguments(\n",
    "    output_dir=\"./results\",\n",
    "    overwrite_output_dir=True,\n",
    "    num_train_epochs=4,\n",
    "    per_device_train_batch_size=16,   \n",
    "    per_device_eval_batch_size=16,\n",
    "    logging_dir=\"./logs\",\n",
    "    logging_steps=10,\n",
    "    evaluation_strategy=\"epoch\",\n",
    "    save_strategy=\"epoch\",\n",
    "    fp16=torch.cuda.is_available(), \n",
    "    save_total_limit=2\n",
    ")\n",
    "\n",
    "\n",
    "\n",
    "#definir el trainer\n",
    "trainer = Trainer(\n",
    "    model=model,\n",
    "    args=training_args,\n",
    "    train_dataset=train_dataset,\n",
    "    eval_dataset=val_dataset,\n",
    "    tokenizer=tokenizer   # para logging\n",
    ")\n",
    "\n",
    "#Entrenar el modelo\n",
    "start_time = time.time()\n",
    "trainer.train()\n",
    "print(f\"Total Training Time: {(time.time() - start_time)/60:.2f} min\")\n",
    "\n",
    "#Evaluar el modelo\n",
    "results = trainer.evaluate()\n",
    "print(\"Evaluation results:\", results)\n",
    "\n",
    "#Perplejidad\n",
    "perplexity = np.exp(results[\"eval_loss\"])\n",
    "print(f\"Perplexity: {perplexity:.2f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "1e6de261",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Truncation was not explicitly activated but `max_length` is provided a specific value, please use `truncation=True` to explicitly truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "What is an exoplanet?\n",
      "\n",
      "An exoplanet is a planet that is born in the sun's disk. its orbit is like that of a star. it is orbiting it, moving its gravitational pull along its axis, and then orbiting it again. the mass of an exoplanet depends on its mass and the planet's gravity, but the mass of an exoplanet is not a fundamental factor in determining its orbit. exoplanets have a mass that is a factor of five\n"
     ]
    }
   ],
   "source": [
    "#Probar que t√°n bien el modelo apredi√≥ del dataset\n",
    "generator = pipeline(\"text-generation\", model=model, tokenizer=tokenizer, device=0 if torch.cuda.is_available() else -1)\n",
    "\n",
    "prompt = \"What is an exoplanet?\"\n",
    "output = generator(prompt, max_length=100, do_sample=True, temperature=0.7)\n",
    "print(output[0][\"generated_text\"])\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "astro_env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
